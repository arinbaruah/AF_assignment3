---
title: "ETC 5550 Assignment 3"
author: "Arindom Baruah (32779267)"
format: html
editor: visual
number-sections: true
execute: 
  warning: false
  message: false
---

# Libraries

```{r}
library(fpp3)
library(tidyverse)
library(tidyverse)
library(kableExtra)
```

# Data

```{r}
#| label: tbl-data
#| tbl-cap: "Glimpse of the data"

set.seed(32779267)
pop <- readr::read_rds("https://bit.ly/monashpopulationdata") |>
  filter(Country == sample(Country, 1))

pop %>% head() %>% kbl()
```

@tbl-data illustrates the timeseries data for the country "Africa Western and Central	AFW" which shall be analysed in the upcoming sections.

# Exercises

## Splitting data into train-test data and fitting benchmark models.{#sec-initial}

Let us first visualise the population growth for the country "Africa Western and Central AFW".

```{r}
#| label: fig-totpop
#| fig-cap: "Population growth in Africa Western and Central AFW"
pop %>% autoplot() + theme_minimal() + 
  labs(title = "Population growth in Africa Western and Central AFW")
```

:::{.callout-note}
# Key takeaway

As we can observe in @fig-totpop, __the population in Africa and Central AFW has grown at an exponential rate__. 

We shall transform our data such that the overall trend of the data is linear. In this regard, __we shall apply a logarithmic transformation of our data.__
:::


```{r}
#| label: fig-totpoplog
#| fig-cap: "Logarithim population growth in Africa Western and Central AFW"

pop_log <- pop %>% mutate(Log_population = log(Population))

pop_log %>% autoplot(Log_population) + theme_minimal() + 
  labs(title = "Logarithmic population growth in Africa Western and Central AFW",y = "Log of population")
```

As we can now observe through @fig-totpoplog, __upon performing the logarithmic transformation, we can observe a linear trend in the growth rate.__



In the next step, we shall split our data into train and test data. The split is formed on the following basis:

- Population data for the years __before 2018__ will be used as the __train dataset__
- Population data for the years __2018 - 2022__ will be used as the __test dataset__

```{r}
pop_train <- pop_log %>% filter(Year < 2018) %>% select(-Population) # Train dataset
pop_test <- pop_log %>% filter(Year >= 2018) %>% select(-Population) # Test dataset
```


Now that we have transformed our data and split it based on training and testing dataset, we will utilise the benchmark models to forecast the logarithmic population growth. The models which will be used are as follows:

- ETS (Error Trend Seasonality) model
- Naïve model
- Mean model
- Drift model

As our data does not contain a seasonality component, hence we shall not fit a seasonal naïve model.

```{r}
model_fit <- pop_train %>% model(ets = ETS(Log_population), # ETS Model
                    naive = NAIVE(Log_population), # Naïve model
                    mean = MEAN(Log_population), # Mean model
                    drift = RW(Log_population ~ drift())) # Drift model
```


Once we have fitted our training data on the ETS model and the benchmark models, we will check the peformance of each of these models. 


```{r}
#| label: tbl-accuracy
#| tbl-cap: "Model performance output metrics"

pop_fc <- model_fit |>
  forecast()

pop_fc %>% accuracy(pop_test) %>% arrange(RMSE) %>% select(-c(ME,MAE,MPE,MAPE,MASE,RMSSE)) %>% kbl()
```

:::{.callout-note}
# Key takeaway

As we can observe in @tbl-accuracy, the __"Drift method"__ is observed to be the model which has forecasted the test dataset with least root mean square error. This is followed by the ETS model, Naïve model and the mean model respectively.

__Hence, based on the RMSE values, we can say that the Drift method foreacasts the best for the test dataset.__
:::


## Checking for the residuals of the Drift model

Let us now check how do the residuals look like for the drift model.


```{r}
#| label: fig-residuals
#| fig-cap: "Residual diagnosis of the predictions from the drift method"

model_fit %>% select(drift) %>% gg_tsresiduals() + ggtitle("Residuals in the drift model") 
```

```{r}
#| label: tbl-lb
#| tbl-cap: "Ljung-Box test results for the model residuals"
# ljung box test

augment(model_fit) %>%
  features(.resid,ljung_box) %>% kbl()
```

:::{.callout-note}
# Key takeaway

Based on the ACF plot in @fig-residuals, we can observe the following key points:

1. The ACF plot illustrates that there is a pattern which we can observe in the residuals of the drift model.

2. The pattern in the ACF plot indicates that the __model has missed out on the trend of the data, as a result of which, the ACF values appear to be higher than the significant thresholds__ (shown by the <span style=color:blue>blue lines</span>).

3. Additionally, we observe that the __residuals of the model fail to form a normal distribution.__

4. Performing the Ljung-Box test on the residuals as illustrated in @tbl-lb indicates that __the drift method has a Ljung-Box P-value (`lb_pvalue`) of less than 0.05, which further indicates that the model has failed the test and could not effectively produce an accurate forecast of the data.__

5. Based on the above observations, we can say that the __residuals of the model do not appear to be simple white noise.__
:::

Let us try to visualise how do our forecasts look like for the training data.

```{r}
#| label: fig-model
#| fig-cap: "Forecasts and the confidence intervals for each model"

pop_fc <- model_fit |>
  forecast(new_data = pop_train)


model_fit %>% forecast(h = "10 years")  %>% autoplot(pop_train) + 
  labs(title = "Forecasted population in Africa Western and Central AFW",
       y = "Logarithmic population") + theme_minimal()
```
As we can observe from @fig-model, __the mean and the naïve models have completely missed the overall trend of the logarithmic population rise__ while the __drift and the ETS models appear to follow the overall global trend very closely.__


## Utilisation of cross-validation and recalculation of RMSE for the models

In this section, we will attempt to perform cross-validation on our training data which will provide us with multiple smaller training data and allow our model to fit better, thereby providing us with more accurate forecasts. The implementation of cross-validation of our data is delineated below.

```{r}
#| label: tbl-cv
#| tbl-cap: "Updated RMSE of each model after performing cross-validation on the training data"

pop_train %>% stretch_tsibble(.init = 15,.step = 1) %>% 
                    model(ets = ETS(Log_population), # ETS Model
                    naive = NAIVE(Log_population), # Naïve model
                    mean = MEAN(Log_population), # Mean model
                    drift = RW(Log_population ~ drift())) %>% # Drift model
                             forecast(h = "5 years") %>% accuracy(pop_train) %>% arrange(RMSE) %>% select(-c(ME,MAE,MPE,MAPE,MASE,RMSSE)) %>% kbl()
 
```

:::{.callout-note}
# Key takeaway

After performing cross-validation of the data and obtaining the model metrics as illustrated by @tbl-cv, __we observe that the ETS model is the one that performs the best followed by the drift model.__ 

This indicates that __with the implementation of the cross-validation, the ETS model is able to train itself better due to the availability of more data and outperform the drift method which was observed to be the best in @sec-initial .__ 

:::
