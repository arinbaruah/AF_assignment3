---
title: "ETC 5550 Assignment 3"
author: "Arindom Baruah (32779267)"
format: html
editor: visual
number-sections: true
code-fold: true
execute: 
  warning: false
  message: false
---

# Libraries

```{r}
#| code-fold: false
library(fpp3)
library(tidyverse)
library(kableExtra)
library(ggplot2)
```

# Data

```{r}
#| label: tbl-data
#| tbl-cap: "Glimpse of the data"

set.seed(32779267)
pop <- readr::read_rds("https://bit.ly/monashpopulationdata") |>
  filter(Country == sample(Country, 1))

pop %>% head() %>% kbl()
```

@tbl-data illustrates the timeseries data for the country "Africa Western and Central	AFW" which shall be analysed in the upcoming sections.

# Exercises

## Splitting data into train-test data and fitting benchmark models.{#sec-initial}

Let us first visualise the population growth for the country "Africa Western and Central AFW".

```{r}
#| label: fig-totpop
#| fig-cap: "Population growth in Africa Western and Central AFW"
pop %>% autoplot() + theme_minimal() + 
  labs(title = "Population growth in Africa Western and Central AFW")
```

:::{.callout-note}
# Key takeaway

As we can observe in @fig-totpop, __the population in Africa and Central AFW has grown at an exponential rate__. 

We shall transform our data such that the overall trend of the data is linear. In this regard, __we shall apply a logarithmic transformation of our data.__
:::


```{r}
#| label: fig-totpoplog
#| fig-cap: "Logarithim population growth in Africa Western and Central AFW"


pop %>% autoplot(log(Population)) + theme_minimal() + 
  labs(title = "Logarithmic population growth in Africa Western and Central AFW",y = "Log of population")
```

As we can now observe through @fig-totpoplog, __upon performing the logarithmic transformation, we can observe a linear trend in the growth rate.__



In the next step, we shall split our data into train and test data. The split is formed on the following basis:

- Population data for the years __before 2018__ will be used as the __train dataset__
- Population data for the years __2018 - 2022__ will be used as the __test dataset__

```{r}
#| code-fold: false
pop_train <- pop %>% filter(Year < 2018) # Train dataset
pop_test <- pop %>% filter(Year >= 2018) # Test dataset
```

<div class="alert alert-info">
üõ†‚öôÔ∏èüî¨‚õìÔ∏è

Now that we have transformed our data and split it based on training and testing dataset, we will utilise the benchmark models to forecast the logarithmic population growth. The models which will be used are as follows:

- ETS (Error Trend Seasonality) model
- Na√Øve model
- Mean model
- Drift model

</div>

:::{.callout-warning}
# Note

As our __data does not contain a seasonality component, hence we shall not fit a seasonal na√Øve benchmark model.__

:::

```{r}
#| code-fold: false
model_fit <- pop_train %>% model(ets = ETS(log(Population)), # ETS Model
                    naive = NAIVE(log(Population)), # Na√Øve model
                    mean = MEAN(log(Population)), # Mean model
                    drift = RW(log(Population) ~ drift())) # Drift model
```


Once we have fitted our training data on the ETS model and the benchmark models, we will check the peformance of each of these models. 


```{r}
#| label: tbl-accuracy
#| tbl-cap: "Model performance output metrics"

pop_fc <- model_fit |>
  forecast()

pop_fc %>% accuracy(pop) %>% arrange(RMSE) %>% 
  select(-c(ME,MAE,MPE,MAPE,MASE,RMSSE)) %>% kable(digits = 2, align = "r") %>%
  kable_styling(full_width = T) %>%
  row_spec(0, color = "white", background = "#FFAF33", bold = TRUE) %>%
  column_spec(1, width = "2.5em", color = "white", background = "#FFAF33") %>%
  column_spec(1:5, width = "2.5em") %>%
  row_spec(3, bold = FALSE)
```

:::{.callout-note}
# Key takeaway

As we can observe in @tbl-accuracy, the __"Drift method"__ is observed to be the model which has forecasted the test dataset with least root mean square error. This is followed by the ETS model, Na√Øve model and the mean model respectively.

__Hence, based on the RMSE values, we can say that the Drift method forecasts the best for the test dataset.__
:::


## Checking for the residuals of the Drift model

Let us now check how do the residuals look like for the drift model.


```{r}
#| label: fig-residuals
#| fig-cap: "Residual diagnosis of the predictions from the drift method"

model_fit %>% select(drift) %>% gg_tsresiduals() + ggtitle("Residuals in the drift model") 
```

```{r}
#| label: tbl-lb
#| tbl-cap: "Ljung-Box test results with lag=10 for the model residuals"
# ljung box test

augment(model_fit) %>%
  features(.resid,ljung_box,lag = 10) %>% kable(digits = 2, align = "r") %>%
  kable_styling(full_width = T) %>%
  row_spec(0, color = "white", background = "#FFAF33", bold = TRUE) %>%
  column_spec(1, width = "2.5em", color = "white", background = "#FFAF33") %>%
  column_spec(1:4, width = "2.5em") %>%
  row_spec(3, bold = FALSE) # Lag = 10 chosen due to non-seasonal data
```


:::{.callout-note}
# Key takeaway

Based on the ACF plot in @fig-residuals, we can observe the following key points:

1. The ACF plot illustrates that there is a pattern which we can observe in the residuals of the drift model.

2. The pattern in the ACF plot indicates that the __model has missed out on the trend of the data, as a result of which, the ACF values appear to be higher than the significant thresholds__ (shown by the <span style=color:blue>blue lines</span>).

3. Additionally, we observe that the __residuals of the model fail to form a normal distribution.__

4. Performing the Ljung-Box test on the residuals as illustrated in @tbl-lb indicates that __the drift method has a Ljung-Box P-value (`lb_pvalue`) of less than 0.05, which further indicates that the model has failed the test and could not effectively produce an accurate forecast of the data.__

5. Based on the above observations, we can say that the __residuals of the model do not appear to be simple white noise.__
:::

Let us try to visualise how do our forecasts look like for the training data.

```{r}
#| label: fig-model
#| fig-cap: "Forecasts and the confidence intervals for each model"

pop_fc <- model_fit |>
  forecast(new_data = pop_train)


model_fit %>% forecast(h = "10 years")  %>% autoplot(pop_train) + 
  labs(title = "Forecasted population in Africa Western and Central AFW",
       y = "Logarithmic population") + theme_minimal()
```
As we can observe from @fig-model, __the mean and the na√Øve models have completely missed the overall trend of the logarithmic population rise__ while the __drift and the ETS models appear to follow the overall global trend very closely.__


## Utilisation of cross-validation and recalculation of RMSE for the models {#sec-final}

In this section, we will attempt to perform cross-validation on our data which will provide us with multiple smaller training data and allow our model to fit better as we will be able to create multiple "5-year" forecasts, thereby providing us with more accuracy. The implementation of cross-validation of our data is delineated below.

```{r}
#| label: tbl-cv
#| tbl-cap: "Updated RMSE of each model after performing cross-validation on the test data"

pop_train %>% stretch_tsibble(.init = 15,.step = 1) %>% 
                    model(ets = ETS(log(Population)), # ETS Model
                    naive = NAIVE(log(Population)), # Na√Øve model
                    mean = MEAN(log(Population)), # Mean model
                    drift = RW(log(Population))) %>% # Drift model
                             forecast(h = "5 years") %>% accuracy(pop) %>% arrange(RMSE) %>% select(-c(ME,MAE,MPE,MAPE,MASE,RMSSE)) %>% kable(digits = 2, align = "r") %>%
  kable_styling(full_width = T) %>%
  row_spec(0, color = "white", background = "#FFAF33", bold = TRUE) %>%
  column_spec(1, width = "2.5em", color = "white", background = "#FFAF33") %>%
  column_spec(1:5, width = "2.5em") %>%
  row_spec(3, bold = FALSE)
 
```

:::{.callout-note}
# Key takeaway

After performing cross-validation of the data and obtaining the model metrics as illustrated by @tbl-cv, __we observe that the ETS model is the one that performs the best followed by the drift model.__ 

This indicates that __with the implementation of the cross-validation, the ETS model is observed to train itself better due to the availability of more train/test data and outperform the drift model which was observed to be the best in @sec-initial .__ 

:::


## Reliability of each of the methods

The timeseries analysis on the logarithmic population data was performed through two methods:

- The first method as delineated in @sec-initial created a single "5-year" forecast by using the test dataset to obtain the model performance metric in the form of RMSE.

- The second method as delineated in @sec-final takes the advantage of creating multiple "5-year" forecasts and testing on each of them to obtain the model performance metric (RMSE). This is done so by making multiple number of smaller training datasets which progressively increase by one observation in each "fold" of the data.



:::{.callout-note}
# Key takeaway

After assessing both methods as described, it becomes evident that the approach __employing cross-validation to forecast future outcomes and furnish us with model accuracy stands out as the more dependable choice.__

This is due to the fact that when we utilise cross-validation, we are creating multiple number of "5-year" forecasts instead of just one and evaluating the model performance based on all these forecasts by obtaining an average metric across all the folds obtained.

As a result of the multiple training sets and the testing sets that are created during the method employing cross-validation, __we obtain a model metric (such as RMSE) which is averaged over all the test sets, thereby providing us with a more accurate and a stable estimate to base our decision on.__

:::


# Reference

1. __fpp3__: Hyndman R (2023). _fpp3: Data for "Forecasting: Principles and Practice" (3rd Edition)_. R package version 0.5,
  <https://CRAN.R-project.org/package=fpp3>.

2. __ggplot2__: H. Wickham. ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York, 2016.

3. __tidyverse__: Wickham H, Averick M, Bryan J, Chang W, McGowan LD, Fran√ßois R, Grolemund G, Hayes A, Henry L, Hester J, Kuhn M, Pedersen TL, Miller E, Bache SM, M√ºller K, Ooms J, Robinson D, Seidel DP, Spinu V, Takahashi K, Vaughan D, Wilke C, Woo K, Yutani H (2019). ‚ÄúWelcome to the tidyverse.‚Äù _Journal of Open Source Software_, *4*(43), 1686. doi:10.21105/joss.01686 <https://doi.org/10.21105/joss.01686>.

4. __kableExtra__: Zhu H (2024). _kableExtra: Construct Complex Table with 'kable' and Pipe Syntax_. R package version 1.4.0, <https://CRAN.R-project.org/package=kableExtra>.



